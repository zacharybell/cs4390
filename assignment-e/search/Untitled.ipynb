{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment E requires students to conduct experiments on an IR system that takes a set of documents and a set of queries and produces labels for each query. These labels are then evaluated by a scoring function. The data set is very small so it is likely that simpler approaches will yield better results. The data is divided into a set of 17 documents for each UTEP CS professor and a set of queries. This is a multi-class (17) and multi-label (3) problem. A few initial observations about the data is that some of the query tokens don't match any of the documents. One example is the query for \"ML\" which would only match \"machine learning\" in d6 and d9. Another interesting thing to note is that most of the query strings use primarily nouns with no stop words. Abbreviations are more common and most non-abbriviated words are combined into queries of two or three words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the size of the dataset, unigram should be an effective method for evaluating similarity between documents and query strings. Unigram is rather simple to implement in comparison to N>1 gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordUnigram(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function substitutes the trigram function in the baseline code and produces a score of 0.191."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram performed worst than unigram. I suspect this is due to the size of the dataset. From observations what query strings score high, the strings that seem to do the best have unique words that match unique words in document examples. Having a set of two unique words that match in a very small dataset is rather unlikely which might explain the decrease in performance. Bigram scored 0.169 on the training set. It is also worth noting that I removed the punctuation from each document because the query strings don't typically use them which would decrease the performance of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordBigram(text, stop_words=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    bigram = CountVectorizer(ngram_range=(1,2), stop_words=stop_words).build_analyzer()\n",
    "    return bigram(text)\n",
    "\n",
    "contents = [ doc.translate(None, string.punctuation) for doc in contents ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the stop words from the documents for my third experiment. I tested this with bigrams because stop words wouldn't impact unigram nearly as much. The removal of these words actually decreased performance (0.147). I expected the removal to increase performance slightly but upon further analysis of the stop words I reached the conclusion that I was using a list that discarded too many relevant words (e.g. 'computer')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best perforce was achieved by using unigram with stemming. This method works very well because for a few cases a query word has the same stem as word(s) in a document but this only gets scored if stemming is performed prior. I was able to achieve 0.199 on the training set and 0.153 on the test set with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-415b22892bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'contents' is not defined"
     ]
    }
   ],
   "source": [
    "def stemming(text):\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "    return ' '.join([ stemmer.stem(t) for t in text.split() ])\n",
    "\n",
    "contents = [ stemming(doc) for doc in contents ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various other interesting methods that can be applied to conduct IR for larger datasets. In this case methods like Deep Semantic Similarity Models can be applied by projecting document text to word embeddings using a siamese neural networks. Word embeddings would also play a greater significance when more examples exist of spatial relations between words. These methods wouldn't work very well on this data but would be worth exploring for similar problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
